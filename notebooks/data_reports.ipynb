{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy Data Query Notebook\n",
    "\n",
    "This notebook queries the Delta tables created by the data ingestion script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c1d3aa63-22b9-4406-85ee-62af4bb244c4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 367ms :: artifacts dl 18ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c1d3aa63-22b9-4406-85ee-62af4bb244c4\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/9ms)\n",
      "24/10/30 09:59:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session with Delta support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Energy Data Query\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to the Delta tables (adjust these paths as per your config)\n",
    "public_power_path = \"/workspaces/baywa-data-pipeline/data/public_power\"  # Replace with your actual path\n",
    "price_path = \"/workspaces/baywa-data-pipeline/data/price\"                  # Replace with your actual path\n",
    "installed_power_path = \"/workspaces/baywa-data-pipeline/data/installed_power\"  # Replace with your actual path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-------+\n",
      "|production_type|          timestamp|  value|\n",
      "+---------------+-------------------+-------+\n",
      "|   Wind onshore|2023-12-31 18:45:00|29554.0|\n",
      "|   Wind onshore|2023-12-31 18:45:00|29062.9|\n",
      "|   Wind onshore|2023-12-31 18:45:00|29185.5|\n",
      "|   Wind onshore|2023-12-31 18:45:00|28968.5|\n",
      "|   Wind onshore|2023-12-31 18:45:00|28369.2|\n",
      "|   Wind onshore|2023-12-31 18:45:00|28610.7|\n",
      "|   Wind onshore|2023-12-31 18:45:00|29241.6|\n",
      "|   Wind onshore|2023-12-31 18:45:00|29230.1|\n",
      "|   Wind onshore|2023-12-31 18:45:00|29299.7|\n",
      "|   Wind onshore|2023-12-31 18:45:00|29385.3|\n",
      "|   Wind onshore|2023-12-31 18:45:00|29085.6|\n",
      "|   Wind onshore|2023-12-31 18:45:00|28724.5|\n",
      "|   Wind onshore|2023-12-31 18:45:00|28758.0|\n",
      "|   Wind onshore|2023-12-31 18:45:00|28762.7|\n",
      "|   Wind onshore|2023-12-31 18:45:00|29124.9|\n",
      "|   Wind onshore|2023-12-31 18:45:00|29548.1|\n",
      "|   Wind onshore|2023-12-31 18:45:00|30055.3|\n",
      "|   Wind onshore|2023-12-31 18:45:00|30108.9|\n",
      "|   Wind onshore|2023-12-31 18:45:00|29666.6|\n",
      "|   Wind onshore|2023-12-31 18:45:00|29475.3|\n",
      "+---------------+-------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Querying public power data\n",
    "public_power_df = spark.read.format(\"delta\").load(public_power_path)\n",
    "public_power_df.createOrReplaceTempView(\"public_power\")\n",
    "\n",
    "# Example SQL query to get average production values\n",
    "average_public_power = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM public_power\n",
    "    LIMIT 100\n",
    "\"\"\")\n",
    "average_public_power.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/30 10:00:17 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 9:>                                                        (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|     production_type|      average_value|\n",
      "+--------------------+-------------------+\n",
      "|               Waste| 1115.4239583333328|\n",
      "|          Fossil gas| 2713.9281249999976|\n",
      "|          Fossil oil| 392.81145833333363|\n",
      "|Hydro water reser...|  75.12604166666678|\n",
      "|Renewable share o...|  92.49791666666657|\n",
      "|               Solar| 1135.5500000000018|\n",
      "|Renewable share o...|  80.11875000000002|\n",
      "|       Wind offshore|  5707.162499999985|\n",
      "|             Nuclear|               null|\n",
      "|Cross border elec...| -6802.878124999999|\n",
      "|Hydro pumped storage| 1050.1208333333345|\n",
      "|          Geothermal|  20.22395833333327|\n",
      "|  Hydro Run-of-River| 2080.4947916666647|\n",
      "|        Wind onshore| 26499.397916666672|\n",
      "|       Residual load|  10486.62499999999|\n",
      "|Fossil brown coal...| 3250.2406250000004|\n",
      "|              Others|  211.0635416666664|\n",
      "|    Fossil hard coal| 1713.7614583333345|\n",
      "|             Biomass|  4531.489583333334|\n",
      "|Hydro pumped stor...|-2010.4166666666622|\n",
      "+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Querying public power data\n",
    "public_power_df = spark.read.format(\"delta\").load(public_power_path)\n",
    "public_power_df.createOrReplaceTempView(\"public_power\")\n",
    "\n",
    "# Example SQL query to get average production values\n",
    "average_public_power = spark.sql(\"\"\"\n",
    "    SELECT production_type, AVG(value) AS average_value\n",
    "    FROM public_power\n",
    "    GROUP BY production_type\n",
    "\"\"\")\n",
    "average_public_power.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-------------------+\n",
      "| price|          timestamp|               unit|\n",
      "+------+-------------------+-------------------+\n",
      "|115.34|2024-10-30 21:00:00|EUR / megawatt_hour|\n",
      "|112.37|2024-10-30 22:00:00|EUR / megawatt_hour|\n",
      "|102.53|2024-10-30 11:00:00|EUR / megawatt_hour|\n",
      "|106.95|2024-10-30 12:00:00|EUR / megawatt_hour|\n",
      "| 160.0|2024-10-30 15:00:00|EUR / megawatt_hour|\n",
      "|193.88|2024-10-30 16:00:00|EUR / megawatt_hour|\n",
      "| 115.0|2024-10-30 09:00:00|EUR / megawatt_hour|\n",
      "|109.68|2024-10-30 10:00:00|EUR / megawatt_hour|\n",
      "|123.54|2024-10-30 05:00:00|EUR / megawatt_hour|\n",
      "|139.68|2024-10-30 06:00:00|EUR / megawatt_hour|\n",
      "| 98.16|2024-10-30 01:00:00|EUR / megawatt_hour|\n",
      "| 98.05|2024-10-30 02:00:00|EUR / megawatt_hour|\n",
      "|111.31|2024-10-30 13:00:00|EUR / megawatt_hour|\n",
      "|126.16|2024-10-30 14:00:00|EUR / megawatt_hour|\n",
      "|192.89|2024-10-30 17:00:00|EUR / megawatt_hour|\n",
      "|159.44|2024-10-30 18:00:00|EUR / megawatt_hour|\n",
      "|100.71|2024-10-29 23:00:00|EUR / megawatt_hour|\n",
      "|100.52|2024-10-30 00:00:00|EUR / megawatt_hour|\n",
      "|133.56|2024-10-30 19:00:00|EUR / megawatt_hour|\n",
      "|122.67|2024-10-30 20:00:00|EUR / megawatt_hour|\n",
      "+------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Querying price data\n",
    "price_df = spark.read.format(\"delta\").load(price_path)\n",
    "price_df.createOrReplaceTempView(\"price\")\n",
    "\n",
    "# Example SQL query to get maximum price\n",
    "max_price = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM price\n",
    "\"\"\")\n",
    "max_price.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|max_price|\n",
      "+---------+\n",
      "|   193.88|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Querying price data\n",
    "price_df = spark.read.format(\"delta\").load(price_path)\n",
    "price_df.createOrReplaceTempView(\"price\")\n",
    "\n",
    "# Example SQL query to get maximum price\n",
    "max_price = spark.sql(\"\"\"\n",
    "    SELECT MAX(price) AS max_price\n",
    "    FROM price\n",
    "\"\"\")\n",
    "max_price.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|year|       total_power|\n",
      "+----+------------------+\n",
      "|2002| 4270.264999999997|\n",
      "|2003| 4270.264999999997|\n",
      "|2004| 4270.264999999997|\n",
      "|2005| 4270.264999999998|\n",
      "|2006| 4270.264999999997|\n",
      "|2007|4270.2649999999985|\n",
      "|2008| 4270.264999999997|\n",
      "|2009|          4270.265|\n",
      "|2010| 4270.264999999997|\n",
      "|2011| 4270.264999999999|\n",
      "|2012| 4270.264999999997|\n",
      "|2013|          4270.265|\n",
      "|2014| 4270.264999999997|\n",
      "|2015| 4270.264999999999|\n",
      "|2016| 4270.264999999997|\n",
      "|2017| 4270.264999999999|\n",
      "|2018| 4270.264999999997|\n",
      "|2019| 4270.264999999999|\n",
      "|2020| 4270.264999999997|\n",
      "|2021| 4270.264999999999|\n",
      "+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Querying installed power data\n",
    "installed_power_df = spark.read.format(\"delta\").load(installed_power_path)\n",
    "installed_power_df.createOrReplaceTempView(\"installed_power\")\n",
    "\n",
    "# Example SQL query to get total installed power by year\n",
    "total_installed_power = spark.sql(\"\"\"\n",
    "    SELECT year, SUM(installed_power) AS total_power\n",
    "    FROM installed_power\n",
    "    GROUP BY year\n",
    "    ORDER BY year\n",
    "\"\"\")\n",
    "total_installed_power.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
